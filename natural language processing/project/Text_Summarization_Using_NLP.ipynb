{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization_Using_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1wJlg-m22H6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7a12c9-3d7e-4ba0-c39c-0b86b0fc9770"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9G2bPtVAdFL",
        "outputId": "f52b0cd0-0ae4-495e-d206-3b66dcc3d664"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEqWgtIQiaP0"
      },
      "source": [
        "#importing all necessary libraries\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import reuters\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk.data\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "#nltk.download('all')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuf0owEAdOy0",
        "outputId": "77b44947-99e4-47d8-a7ac-93470add6269"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJLZctTCkcSh"
      },
      "source": [
        "summary_threshold = 5  # number of sentences in final summary\n",
        "stop_words = stopwords.words('english') #getting all the stopwords\n",
        "stemmer = PorterStemmer()  #using PorterStemmer for stemming purposes"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKLzFRRIlXMY"
      },
      "source": [
        "class Summarizer():\n",
        "    #taking in articles and extracting the headline and body to place them in a list\n",
        "    def __init__(self, article): #constructor\n",
        "      self._articles = []\n",
        "      for doc in article:\n",
        "            with open(doc) as f:\n",
        "                headline = f.readline() #first line of article is the headline\n",
        "                url = f.readline() #second line of article is the url\n",
        "                body = f.read().replace('\\n', ' ') #read the remaining of the article and replace the empty lines with a whitespace\n",
        "                #if headline and body is not empty, then we assign the values into 'articles' list\n",
        "                if not self.valid_input(headline, body):\n",
        "                    self._articles.append((None, None))\n",
        "                    continue\n",
        "                self._articles.append((headline, body))\n",
        " \n",
        "    #check if headline and body has any text or not\n",
        "    def valid_input(self, headline, article_text):\n",
        "        return headline != '' and article_text != ''\n",
        "\n",
        "    #perform tokenization and stemming (using PorterStemmer)  \n",
        "    def tokenize_and_stem(self, text):\n",
        "        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "        filtered = []\n",
        "        #filter out numeric tokens, raw punctuation, etc.\n",
        "        for token in tokens:\n",
        "            if re.search('[a-zA-Z]', token):\n",
        "                filtered.append(token)\n",
        "        stems = [stemmer.stem(t) for t in filtered]\n",
        "        return stems\n",
        "\n",
        "    #building tf_idf score in a matrix for each word in the Reuters corpus\n",
        "    def build_TFIDF_model(self):\n",
        "        token_dict = {}\n",
        "        #getting the name of the article\n",
        "        for article in reuters.fileids():\n",
        "            token_dict[article] = reuters.raw(article) \n",
        "        # Use TF-IDF to determine frequency of each word in our article, relative to the word frequency distributions in corpus of 10.8k Reuters news articles.\n",
        "        self._tfidf = TfidfVectorizer(tokenizer=self.tokenize_and_stem, stop_words='english', decode_error='ignore')\n",
        "        tdm = self._tfidf.fit_transform(token_dict.values())\n",
        "\n",
        "    #extracting each sentence from the article\n",
        "    def split_into_sentences(self, text):\n",
        "        #identify where each sentence ends\n",
        "        tok = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "        sentences = tok.tokenize(self.remove_smart_quotes(text))\n",
        "        #consider sentences with a length of more than 10\n",
        "        sentences = [sent.replace('\\n', '') for sent in sentences if len(sent) > 10]\n",
        "        return sentences\n",
        "\n",
        "    #function for generating the summary\n",
        "    def generate_summaries(self):\n",
        "        #Identifying the number of sentences\n",
        "        total_num_sentences = 0\n",
        "        for article in self._articles:\n",
        "            total_num_sentences += len(self.split_into_sentences(article[1]))\n",
        "        \n",
        "        #If article is shorter than the desired threshold, return the original articles\n",
        "        if total_num_sentences <= summary_threshold:\n",
        "            return [x[1] for x in self._articles]\n",
        "\n",
        "        self.build_TFIDF_model()  #build tf-idf score matrix from Reuters corpus\n",
        "        self._scores = Counter()\n",
        "        for article in self._articles:\n",
        "            self.score(article)\n",
        "        highest_scoring = self._scores.most_common(summary_threshold)\n",
        "\n",
        "        # Appending highest scoring \"representative\" sentences, returns as a single summary paragraph and save to text file\n",
        "        with open(\"Summary.txt\", \"w\") as fwrite:\n",
        "          fwrite.write(\"Headline: \\n\")\n",
        "          for article in self._articles:\n",
        "            fwrite.write(article[0])\n",
        "          fwrite.write(\"\\nSummary: \\n\")\n",
        "          fwrite.write(' '.join([sent[0] for sent in highest_scoring])) \n",
        "        return ' '.join([sent[0] for sent in highest_scoring])\n",
        "    \n",
        "    #remove smart quotes\n",
        "    def remove_smart_quotes(self, text):\n",
        "        return text.replace(u\"\\u201c\",\"\").replace(u\"\\u201d\", \"\")\n",
        "\n",
        "    #this function is to assign the sentences in the summary based on its relevance to headline, length, sentence position, word frequencies\n",
        "    def score(self, article):\n",
        "        headline = article[0]\n",
        "        sentences = self.split_into_sentences(article[1])\n",
        "        frequency_scores = self.frequency_scores(article[1])\n",
        "        for i, s in enumerate(sentences):\n",
        "            headline_score = self.headline_score(headline, s) * 1.5\n",
        "            length_score = self.length_score(self.get_tokens(s)) * 1.0\n",
        "            position_score = self.position_score(float(i+1), len(sentences)) * 1.0\n",
        "            frequency_score = frequency_scores[i] * 4\n",
        "            score = (headline_score + frequency_score + length_score + position_score) / 4.0\n",
        "            self._scores[s] = score\n",
        "\n",
        "    #in this function sentences are scored as the sum of their TF-IDF word frequencies.    \n",
        "    def frequency_scores(self, article_text):\n",
        "        # Add our document into the model so we can retrieve scores\n",
        "        response = self._tfidf.transform([article_text])\n",
        "        feature_names = self._tfidf.get_feature_names() # these are just stemmed words\n",
        "        word_prob = {}  # TF-IDF individual word probabilities\n",
        "        for col in response.nonzero()[1]:\n",
        "            word_prob[feature_names[col]] = response[0, col]\n",
        "\n",
        "        #taking each sentence score based on its word probability\n",
        "        sent_scores = []\n",
        "        for sentence in self.split_into_sentences(article_text):\n",
        "            score = 0\n",
        "            sent_tokens = self.tokenize_and_stem(sentence)\n",
        "            for token in (t for t in sent_tokens if t in word_prob):\n",
        "                score += word_prob[token]\n",
        "\n",
        "            # Normalize score by length of sentence, since we later factor in sentence length as a feature\n",
        "            sent_scores.append(score / len(sent_tokens))\n",
        "        return sent_scores\n",
        "\n",
        "    #this function gives sentence a score between 0 to 1 based on percentage of words common to the headline and the article\n",
        "    def headline_score(self, headline, sentence):\n",
        "        title_stems = [stemmer.stem(w) for w in headline if w not in stop_words]\n",
        "        sentence_stems = [stemmer.stem(w) for w in sentence if w not in stop_words]\n",
        "        count = 0.0\n",
        "        for word in sentence_stems:\n",
        "            if word in title_stems:\n",
        "                count += 1.0\n",
        "        score = count / len(title_stems)\n",
        "        return score\n",
        "  \n",
        "    #this function gives sentence score between (0,1) based on how close sentence's length is to the ideal length. \n",
        "    def length_score(self, sentence):\n",
        "        len_diff = math.fabs(20 - len(sentence))\n",
        "        return len_diff/20\n",
        "      \n",
        "      \n",
        "    #split each sentences into tokens\n",
        "    def get_tokens(self,text):\n",
        "      lowers = text.lower()\n",
        "      no_punctuation = lowers.translate(string.punctuation)\n",
        "      tokens = nltk.word_tokenize(no_punctuation)\n",
        "      return tokens\n",
        "\n",
        "    #this function gives a value between (0,1), corresponding to sentence's position in the article.\n",
        "    #Assuming that sentences at the very beginning and end of the article have a higher weight. \n",
        "    def position_score(self, i, size):\n",
        "        sent_position = i / size\n",
        "        if 0 < sent_position <= 0.1:\n",
        "            return 0.17\n",
        "        elif 0.1 < sent_position <= 0.2:\n",
        "            return 0.23\n",
        "        elif 0.2 < sent_position <= 0.3:\n",
        "            return 0.14\n",
        "        elif 0.3 < sent_position <= 0.4:\n",
        "            return 0.08\n",
        "        elif 0.4 < sent_position <= 0.5:\n",
        "            return 0.05\n",
        "        elif 0.5 < sent_position <= 0.6:\n",
        "            return 0.04\n",
        "        elif 0.6 < sent_position <= 0.7:\n",
        "            return 0.06\n",
        "        elif 0.7 < sent_position <= 0.8:\n",
        "            return 0.04\n",
        "        elif 0.8 < sent_position <= 0.9:\n",
        "            return 0.04\n",
        "        elif 0.9 < sent_position <= 1.0:\n",
        "            return 0.15\n",
        "        else:\n",
        "            return 0  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOuCjfEJjN6U",
        "outputId": "e1e8d18a-60a2-450d-c7e5-351a4b6549fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6GTP97QjSDT",
        "outputId": "6cb55d25-7c83-4857-d15a-b4d998f1d91b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9IBeKvBuheu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09e56941-4b03-4cab-ff9b-94465d687fa9"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/nlp project/')\n",
        "article = [\"article1.txt\"] #pushing article into a list\n",
        "summarized_article = Summarizer(article) #Passing the article into the 'Summarizer' class\n",
        "s = summarized_article.generate_summaries() #Print the summary\n",
        "print(s)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-363879b06527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"article1.txt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#pushing article into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msummarized_article\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Passing the article into the 'Summarizer' class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarized_article\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Print the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3d2fdf18ddc9>\u001b[0m in \u001b[0;36mgenerate_summaries\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_articles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_TFIDF_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#build tf-idf score matrix from Reuters corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_articles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3d2fdf18ddc9>\u001b[0m in \u001b[0;36mbuild_TFIDF_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtoken_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#getting the name of the article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mtoken_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Use TF-IDF to determine frequency of each word in our article, relative to the word frequency distributions in corpus of 10.8k Reuters news articles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    }
  ]
}